Installing jupyter notebook on your ec2 instance--
on putty terminal cmd prompt------

ubuntu@ip-172-31-46-57:~$ sudo apt-get update

Next installing jupyter notebook--

$sudo apt install jupyter-notebook

$jupyter notebook --ip 0.0.0.0 --no-browser

Now the jupyter notebook has been started.JUst copy the givn url
and replace 127.0.0.1 with your instance public ip.
Copy this onto browser url.So,go backto ur ec2 running dashboard and look for public ipv4 address.
just copy that-- and on your browser and replace with this.


press enter--remember toput with http  and not https.

Amazon machine images--
So,1st stepis to select ami-amazon machine image which is ubuntu inour case.
So,now wewill discuss ami in detail--
lets say u want to work on a linux instance. and u installjupyter notebook. and then u install few more libraries like pandas ,numpyetc.
and u want someone in urteam to do the experiments. Now, that person has tocreate that same environment.And many times the environment 
that u want to replicate is much more complex.
So,the ans iscreate an imageofinstance that u want.
This image contains all the info tolaunch an instance.

Features of AMI-
provides all the pre-installed packages needed. Faster boot time. AMI takes your S3 space.
By defualt all the imagesthat u createare private tou.U cnmakethempublic.

How to create AMI's--
1st goto ec2 dashboard --instances--actions--gotoimage and templates-- create image---3 options --
imagename--jupyer notebook AMI
2nd option is optional
3rdno reboot--what happens amazon ec2 shutsdown the instance,take snapshots when u attach volummes,
creates and registers the ami and then reboots theinstance.So u can select no reboot to avoid having ur instance shutdown.

add volume is again optional.

click create.

Onceitscreated--uwillbetakenback tourinstance dashboard.

So,inthe navigation bar on the left u will see instance bar and u see images-->ami-->click on that--

Onceit is created,u can follow the same instance as u did for ubuntu machine.
name this ami--like office_laptop_ami

goto ec2 dashboard--
launch instance
In the navigation bar-- there is my amis--click on that--

do allthe steps.Once u willlaunch thisinstance.Jupyer notebookis already installed.

Compute servicesin AWS--
1.Amazon lightsail or AWS lightsail--This is a lightweight virtual private servr.Here u get ssd storge,memory and sanpshots to
protect ur data.
Developers whoare looking for testing enviroor statrtups can use this.
2.AWS Batch--For batch processing and scaling.U donot need toinstall any batch processing s/w.
foreg letssay ur app process data stored in postgre and dynamodb everynight at midnight.So u
define a jobin awsbatch and it executes whatever u define and it needs other computing resources like
ec2 for that purpse but u donot need toworry about that.Soafter the data is processed
u can analyze it and storeit.
3.elastic beanstalk--this is a PAAS .u can use it to create web services.And it supports
multiple lans like java,php,node.js,ruby,go.
U just need toupload ur code and beanstalk will deploy ur code,need not worry about challengs like scaling.
4.AWS lambda--It is a serverless computing in aws.Again u dont need to manage any resources.
U just need to uploadur code. U can useit to create apis.Trigger ur code from other apis.
5.Serverless app repository--It is a repositrory where u can search forpre-built application.
Lets say u want an app for rewriting an email.So instead of rewriting app--u can search for it in repository 
and configure it accr to ur needslike changing parameter variables etc.
6. AWS outposts--Sometmes ur apprequire very high latency.Soin that case,
u want to process ur data locally.In such case u can use AWS outposts.It gives exact experienceonpremises as 
on cloud.

Networking--
Basic conceptsof networking part1--

public and private n/w--
public n/w is simply internet.Anyone aroundthe world can access it without any restriction.
privaten/w--only the concerned people cn acecess it.

Now,when we have many n/ws we need to send data from 1 n/w toanother.
For that we need a deviceknown as gateway.It also servesas astopping point when data travels from1 n/w to another
Internet gteway is used toconnect to internet.

If u have a wifi router at home,then ur internet gateway is the modem of ur rouer that isd provides.

IP Address--it is a unique addr that identifies a device on the internet or on a local n/w.
eg 192.158.2.20
where each part is of 8 nits. soit can range from0.0.0.0 to 255.255.255.255
This notation is fro ipv4 addressing.We also have ipv6 addressing.
This ip  addr has 2 parts. 1 identifies the n/w and the otherr host.
based on this host part,ip addr can be dividedinto multiplecalsses.
and each of these classses has amultiple range of ip addresses which defines a rangeof devices 
u can have in ur n/w.
class A--1st 8 bit n/w rest 24 host.It can have more than 16million hosts.
class B-16 bits for n/w,next 16 bits for hosts. This can be used formedium to large size nw and can have around 65000 hosts.
class C--1st 24 n/w rest 8 for host.It is used for samll size local area n/w.

Categories of ipaddr--
public ip addr and private ip addr--
public ip addr is the addr which all devices outside ur n/w used to recognize ur n/w .
private ipaddr can be used only frominside the n/w.

Public ip addr is divided into -
dynamic and static ip adddr--
dynamic ip addr keeps on changing automatically on a reg basis.
ISP providers buy large amount of ipaddresses .
Static ip addr are constants.

Subnets--Inside ur n/w u can make several divisions of ur n/w.The subn/ws inside ur larger n/wsare known as
subnts.Creating subnets aer very useful as ur n/w traffic can travel a shorter dist without passing
unnecessary routers to reach its destination.

Lets say we have 6 divisions of our n/w and we need to send a pkt to a subnet4.
So instead of scanningthe completen/w,ur n/w pkt will directly goto subnet4.

U can createmultiple private and public n/ws.

CIDR--classless inter domain routing--
It is an IP addressing scheme that improvesthe allocation of IP addresses and replaces the oldsystem based on
classes A,B,C.
The challenges with old class system is no.of hosts.
eg classA with16million+ hosts, class B-65,535 hosts an class c-254 hosts.

Suppose we are currently working witth class C and we have 250 hosts-->we neeed 50more hosts i.e. 300 hosts-->then--u do this and u fallintoa classb n/w.
and 65235 hosts are waisted.

cidr ip addressing lookslike-- 
192.158.2.20/20===>before / is ipaddr-- after / is no. of bits of n/w addr.
i.e. 1st 20 bits for n/w and rest 12 for host.

So,if we convert these numbers into binary then--
1100 0000.1001 1110.0000 0010.1100 0000 
------------------------
           |
           V
          n/w            -------------- host

Route tables-- these contains the set of rules and necessary info to fwd a pkt 
along the best path towards its destination where each pkt contains info regarding
its origin and destination.

SSH--Secure shell- it provides a connection tou sou can connect toany virtul m/c.
such that u can write cmds in urlocal m/c and it gets executedin the server.
To establish an ssh connec,a remote m/c must be running a s/w known as ssh daemon.
andthe user should must have ssh client s/w.

VirtualPrivateCloud(VPC)--
Amazon VPC lets us create ourown private n/win aws.It is aservicethat lets us launch aws resources
in logically isolated virtual n/w that we define.
We can have completecontrolover our virtualnetworking envio including selectionof our own ip addr range,
creation of subnets and configuration of route tablesandn/w gateways..
Inside our cloud,wecan create public subnets and connect them tothe internet gateway sothat the resources inside
the public subnet can be accessed through the internet.
wecan alsocreate multiplesubnetsinside our vpc.letssay 1 public, another privatesou can hosts 
ur websitesintopublic and database and other monitoring tool in your privaten/w.

Nowcreating vpc inside aws--
it will have public and privatesubnet. And for each the subnets we will create a route tables and then we 
willlaunch an ec2 instance inside both the subnets.
We will attach internet gateway with the public subnet so that we canconnect to that subnet using the internet.
Now,t connect to private subnet, we willconnect public ec2 instance with private ec2 instance 

How to creat VPC--

step 1-- create a vpcloud--
suppose we want tocreate a n/w for 1000 devices.
20.0.0.0/?--- 2^10=1024
s0,weneed atleast 10 bits. and rest 22 for n/w.tf
20.0.0.0/22
So,goto aws mgmt console-- and completing step1--
goto aws mgmt console---search for vpc--click on 1st option--
See on left side pane-- clickon ur vpcs--aws bydefault gives vpc.We wont be using that.We will be creating our own
On rhs click create vpc--u get diff text fields--
nametag-- office_laptop-vpc
ipv4 cidr--20.0.0.0/22-----this is a randomip addr.U can change itaccr to ur requirements.
click create vpc.

Step 2--creating subnets--dividing this n/winto 2 parts--public and private.

goto--
https://www.davidc.net/sites/default/subnets/subnets.html

n/w addr-20.0.0.0,mask bits--22--update--ok
click divide-- adn we got 2 subnets.--so we can make 1 of them publicanother one private.

Just clickon your vpc on left to head back to vpc dashboard

Now,click on subnets--3 subnets are there. bUt we willcreate our own.

so,click create subnet on top right--
vpcid-- what we just created i.e. office-laptop_vpc

subnet name-- public-subnet

ipv4 cidr blockk-- go to the site--select 1st one i.e 20.0.0.0/23.click create subnet

Same steps for creating privaten/w--
office_laptop_vpc
private_vpc
20.0.2.0/23--
create subnet

Ste 3-- creating roue tables--it contains set ofroutes to fwdthe pktalong thebest route aailable todeleiver todestination.

On the lhs-- route tables--create route tables--
name--public_route_table ,office_laptop_vpc--click create route table.

Now,associating this with a subnet--scroll down a little bit--click subnet associations--
click--edit subnet assocaiations--click public subnet--save associations.

remeber--we can connect a single route table to multiple subnets,butin a single subnet we have just 1 route table.

Now, from left pane--hit route tables agian--
samesteps-- click create route table --....--select private_vpn

Head back to route tables.

4. create internet gateway--
We will create this and connect it to public n/w.

On the left --create internet gateways-- 
name--office_laptop-internet_gateway--create internet gateway

clickon actions--clickon attach to vpc-- select office_laptop_vpc--atachinternet gateway

Next connectthe internet gateway tothe public route table.
So head to route table in left pane--select public route table--clickon routes-->clickon edit routes--->
add route---0.0.0.0/0--select internet gateway that we created---save changes

Internet gateway connected topublic subnet

Now,we willlaunch 2 ec2 instances--1in public another in private.

So,head back to instances-- click launch instances---select amazon linux 2 kernel--
under n/w --office_vpc,
subnet--public subnet
auto-assign public ip--click enable

keep everything else same--

for key pair-- select linuxkeys--clicki acknowledge--
Now,go back to instances dashboard--
rename this to public_instance---

Now,creating privateinstance--same steps--with changes--
n/w --office_laptop_vpc 
subnet--private
auto-assign--disable

key pair linuxkeys---

--launch instance---
view all instances--rename it to private_instance

Now,click on public_instance--scroll down alittle--in details u have both 
public ipaddr and private ip addr--
and for private_instance--nopublic ip adddr.

Now,lets just 1st connect tothe public instance--select that and click connect--click connect again---
and the public instance is launched in the browser itself.

keep this window open.
go backto instance dashboarrd--click onprivate instance and connect tothis aswell---warning--now how to connect toa private instance?
Goto the new window where public instance is launched.
$ vi key.ppk
press enter insert
copy linuxkeys.ppk to this file
esc
:wq
our key.ppk file created.

$chmod 400 key.ppk

Now,the permissions of fileis changed.Now, wecan use ssh to accessour instance fromthe private n/w. so go back to 
connect to instance aws page--Go to ssh client--just copy the ssh -i "linuxkeys.pem"....
and paste it on the terminal  

Network Access Control List--Now we will discuss about the security of this n/w.Andforthis we have n/w access control list--  
ACL is a virtual firewall that controls inbound and outbound traffic at the subnet level.
Whenever we send a request toan ec2 instance-- we doit inthe form of pkts.
when this pkt enters into the n/w-- a firewall known as nacl--evaluates the pkt.This naclhas somepredefined rulesassociatedwith it.
We can have nacl atthe subnetlevel.This nacl works on the concept of stateless packate filtering.
NACL checks both incoming and ougoing pkt.

Few rules which are defined are--
inbound,
rule number,  
protocol/port range
source
allow/deny
eg pkt is inbound,rule number-100,TCP/8888,109.20.30.33/allow
2nd rule--
inbound,110,ssh/22,anywhere,allow
3rd 
inbound,120,http/8883,109.10.30.33/deny
4th
inbound,140,ssh/22,anywhere,allow

pkt comes from--
ssh/22 100.3.33.33--1st rule doesnt satisfy.
so chk 2nd. satisfies
another pkt from http/8883 109.10.30.33
deny

How tocreate NACL--
search for vpc--
on the lhs--scroll down and see--security--network acls--
--create network acl---name it--office_laptop_acl
select office_laptop_vpc--create nw acl

just click on it--scrolldown little--inbound rules--edit inbund rules--addnewrule--
rulenumber lets say 100..so

100,custom tcp,port range-8888,allow
to block a specific ip addr--
clickadd new rule--
101,custom tcp,port-22,source-22.0.0.0/0 ,deny---save changes

U cn do samething for outbound rules.
just checck foroutbound rules

Security groups--A security group is a virtual firewall that controls inbound and outbound traffic of an ec2 instance.

Stateful pkt  filtering--here firewall remebers the active connections.Once the pkt is gone through the security check,
it doesnt has to go again and again for security check.
here,u can define ruleslike--
inbound,allow,9090,109.20.30.33
u can also define protocollike whether its from ssh,http etc
inbound,allow,22,anywhere--
outbound,allow,8888,109.20.30.33
outbound,allow,9090,109.20.30.33 

here we can only define allows and not deny

By default when u create a security instance,a security groupis attatched 
tothe ec2 instance.i.e. it will not allow any n/w pkt from anywhere.
and outbound rules allow all outbound traffic.
u can add upto 60 inbound and outbound rules to the instance.

Creating security groups--
security groups provide instnce level security while nacl provide n/w level security

clickon ec2--
scroll lhs-- n/wand security-->securiy groups---
now,everytime we launch an instance,aws generates security group by default.---create security group--
name- office_laptop_security_group
descripion--allows jupyter connections  
under vpc-- select our vpc.
inbound rule-- addrule--
customtcp,8888,anywhere ipv4--add rule
customtcp,22,myip

lly,u can create outbound rules--create security groups

Shared responsibility model--
Security services available in aws---
its a shared responsibility model.So we as a cloud developer are responsible for security in the cloudand 
security of the cloud.
AWS is responsible for protecing all the hw/aws global infrastructure.
It is responsible for securing all theregions,availability zones , edgepoints.
This infrastructure is composd of both h/w and s/w like-
where s/w like-- compute , storage,database,networking
We as acloud developer are responsible for security ofclients data.
give the access tothe right person whodoes't violate ur data policies.

so,cloud developer takes care of--
customer data
platform,applications,identity and access mgmt
os,n/w and firewallconfiguration
client side encryption,server side encryption,n/w traffic protection

Identity and access mgmt--
When u created ur aws instance, u were given credentials of a root account.
using that root account,u can access all the services in the aws like compute resources, storage,db,networking

Now,u have multiple teamslike data science team,technology team,accounts team

In ur aws environment u might be running some gpu instances,
s3 buckets,databases like dynamodb,aws lambda & apisand billing dashboard.
And we donot want allofthemto have same login and pwd to log into ur account.

Soinstead of giving root level access to everyone, we can define access to each mmber of teamby Identity and Access Management(IAM)
So u can create IAM mgmt account for each 1 of team members
And using this u can handle access of ur aws account at no extra cost.
Fine grained access control
no extra cost
no permissions by default.--but u can change the access.

For eg-- for data science team -- u can give access of gpu instances and dynamo db
tech team to s3 buckets,dynamo db,aws lambda and apis
accounts team--billing dashboard.

For this u require,policies and permissions--
2 types of policies are--
identity based policies--attached toiam account andtells what that particular id can do in aws environment.
resource based policies--who can have the access to the resource and what actions they can perform.

Iam policy looks something like a json.eg--
{ "version":"2012-10-17"
  "Statement":[
      {
	"Effect":"Allow",
	"Action":["s3:*",
		  "cloudwatch:*",
		  "ec2:*"
		],
	"Resource":"*"
      }]
}

so this is telling about all te services allowed.

Identity based policies are of 2 types--
AWS managed policies--It has pre defined policies for complete ec2and anymore.
Inline policies--wehere u can define ur customized policies if u require.
With IAM policy u get the permission tocreate a permissionboundary.
It is an optional feature for themanaged policy.

How tocreate IAM user account--
Sogo to AWSconsole-- search fro IAM--IAM dashboard----click users from left pane--
add users--
put name--
programatic access is whenu want to access using aws using cmdline,etc
tick aws mgmt console access-- i want to create IAM user

username:parinita
fornow,select both of them--
custom pwd:pawsSilver@19
u can deselect reuire pwd reset.  
 
resource based policies. 

Under set permissions-- click on 3rd option--attach existing policies directly
search--ec2fullaccess---select that--next---
Skipping add new tagfornow---

create user

to change permissions lets say u want to change permissions--
click on name of user-- scroll down--add permissions--attach policies directly--
search s3 full-- add permissions--

Iam policies--
creating customized inline policies--
1st is version--which version of policy lang we are using--
to check the latest version of the policy lang-- 
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_version.html

"Version": "2012-10-17"

then is stmt--it can contain single or an array ofstmt.It si the main element of the policy where we define the stmts.
"Statement":[{},{},{}]

in each of the stmt-- we have the 1st element known as "Effect":"Allow"
it specifies whether the stmt results in an allow or a deny.
Next is Action-here we define the specific action or actoins that will be allowed or denied.
"Action": "service-prefix:action-name"
for eg--"ec2:StartInstances"----allows user to start any instance
"s3:GetObject"---- user gets the permisssion of reading object fromthe s3 bucket.

or u can simply write actions as  well. But this is case sensitive and should be written exactlyin awslike--
["ec2:StartInstances","s3:GetObject"] 
"Resourrce":"amazon resource names"---arn string
condition is optional like--
"Condition":{"{condition-operator}":
		{"{condition-key}":"{condition-value}"
		}}
so finally it lookslike--
{ "version" : "2012-10-17",
  "Statement" : [{
		"Effect" : "Allow",
		"Action" : "ec2.*",
		"Resource" : "amazon resource names",
		"Condition":{
  "DateGreaterThan" : {"aws:CurrentTime":"2020-04-01 T00:00:00Z"},
  "DateLessThan" : {"aws:CurremtTime":"2020-06-30 T23:59:59Z"},

}

}]
}

How to create IAM policy---
custom policy----
we want to give fullec2 access to the userbut dont want the user tolaunch an instance in lets say
hongkong region--
Now,goto iam dashboard--click policies--- 
click create policy on top right---click on choose apolicy---search fro ec2 and select 1st option
click all ec2 actions--
then clickon resources--click all resources--
request conditions--add condition--search awsrequestedregion
qualifier--default
operator--stringNotEquals
then value is the code for the hongkong region.
the code for it is--ap-east-1

next--next review--
review policy--
name--HK-Policy
descri-- doesnot allow HK access.
----create policy---

Now,how touse this policy-- goto users--
u can either add a new user or use existing user--

clickonparinita--
the 2 default policies arethere.remove them fornow.

click add permissions toaddcustompolicy--
attach policies directly--search hkpolicy--next---
---add permissions---
lets see an action now-- go back tousers--select username

click on the ohio top right---

IAM Groups--
For big team-- u can use IAM Groups--
u can define diff groupslike-- datascience group,tech group, accounts group--Define permisions or attch policies.

Define Iam user account--select the groupand done. U donot need to define
policies again and again.
A single user can be added intomultiplegroups as well.

How tocreate IAMgroups--wehave 100sor1000sof user towhomwe want to assignsame policy.
search fro IAMdashboard--
left pane--user groups--create group--
give group  name--office_people--u can even add users tothe group--
attach permissions--hk policy--create group

So, whenever anew user comein-- u can attach user tothis group--and it hasthose policies.

So,just clickusers from left pane--user name: lakshya---consolepwd--autogenerated--next permissions--
select add user to group---select office_people group--next--tags optional--create user--

Security Service-- AWS Shield and WAF--
When a bad actor or hacker tries to findout vulnerabilities in our system and violates them,
so that all the authorized users doesn't getthe response back.
So,they use multiple systems and send millions of req so that the system gets busy sorting those req.
This type of attack is known as distributed denial of service attack.
So to protec fromany knid of attck, we have aws shield-- it is a service that protects applications against DDoS attacks.
On aws it is +nt in 2 forms--
shield standard and shield advance.
sheld standard is a free version-- it protects all customers at no cost and protects ur aws resources from the most common types of DDoS attacks.
It uses variety of automated tech to detect malicious traffic in real time and automatially mitigates it.
Shield advanced is a paid srvice-- tat provide detailed attack diagnostics.
It gives 24*7 access t the aws DDoS response Team(DRT).

AWS Web Application Firewall(WAF)--
WAF is a web app firewall that lets u monitor n/w requests that comeinto ur web apps.
It can protect from sql injection attaclks,cross-site scripting,length of requests,
ip addre of the requ,geographical location,regex

Waf cannotbe associated with the ec2 instance.They ca be associated with load balancer,cloudfront

Security Services:KMS and AWS inspector--
Key Mgmt Serviceon AWS-- wee want data tobe secure, both when data is travelling i.e data in rest and when stored i.e. data at rest.
To secure data,encryption is doone,.
AWS KMS provides u with centralized control over the lifecycle and permissions of ur keys.
it provides-- flexibility with accesscontrol,temporarily disablekeys,20000 free aws keymgmt service requests each month.

AWS Inspector--in aws environment u can have many services running ata the same timelike-
ec2instances,s3 buckets,dynamodb,aws lambda and apis, load balancers
It goesthrough aws envir and run multiple automated security tests,
lit down all the possible threats.The security team goes through this 
and resolv the possible issues.

#-------

Storage in aws--  
This is used to retain digital data.So wewill seetypes of storage available on cloud and when tous which one.
block level storage--here storage systemis divided into multiple blocks,So large file is divided into multiple blocks.
and is stored intothe blocks as perthe block size.
Nowif u make any changes to the file, it will only return back the updated parts to the respecive blocks.
So,if ur app requuires lots of updationin  the file,then tis is a good choice.

justlike ur pen drive,u create snapshot of ur data on m/cA which can be recreated in m/cB.

--by amazon ebs
Filelevel storage--
wecan create foldersinside folders and can store data in a hierarchical structure.
dif types of files can be stored and we cn customize these folders permissions.
Multiple vms cn share the samefilesystem.

--amazon efs


Object level storage--each data is broken into objects which is along with its metadataand key isstored .
These objects can be of any type--image file,media file,staticwebsites,etc.
Here when we make changes in a file,a new vrsionof the object is created.
every object has url which  has some permissions  
--amazon s3

Amazon elastic block storage--EBS--
when we workwith ec2 instance werequire cpu,memory,n/w , storage. If ur instance is in running state everuthing works normally.
but when u stop this ec2 instance,u can nolonger access the data stored in storage.This is where amazon ebs comes.-elasticblock storage.
u can connect it to any ec2 instance. When ur instance is in running state,u can store data in it just like ususal.
now,even if u stop this instance, cpu,memory and n/w is lost but ebs volume will not loose data.
ucan start any new ec2 instance and connect it to this ebs.

So, ebs are virtual harddrives,ebs volume can persists between stops and starts of an ec2 instance.
we can define the size type and configurations of te volume that we need.
snapshots are incremental backups of data.

How touse ebs on aws?
just creat few instances like my-instance-1 and my-instance-2

goto ebs on left pane--->volume--->attach vol---->
generalpurposessd is default--->size 10gb--->availabilty zoneshould be same as 
instance zone--->create vol--->name this vol1--->
its state is available--->now we need toattach it with instance1--
select vol1--->actions--->attach vol--->vol--instance--intance1---device let it--->attach
state ischange toinuse.
so goto intance--->select instance1---->storge-->newstorage attached.


Note new volumes are raw block devices and we have to create a filesystembefore we canmount it to user.
So,lets,connect instance1-->ec2 instance connect--->new window--->
$ lsblk ---list allthe blocks
Now u want touse a cmd to get info abut specific device, sush as its filesystem and
if the o/p of that cmd gives data then it means there is nofilesystem onthe device.
$ sudo file -s /dev/xvdf
o/p--/dev/xvdf: data
Sofirst ofall we need tocreate a file system.
$ sudo mkfs -t xfs /dev/xvdf   
Next we want tocreate a mount point
$ sudo mkdir/data
We name the mount point data
Now,weneed a cmd tomount the vol and dire created in previous step
$ sudo mount /dev/xvdf /data
$ lsblk
now creating some random data and put it into dir that we created.
$ cd /data
$ sudo mkdir parinita
$ sudo touch sample.txt
$ ls
NOwgo back to instance and lets detach the volume that we have just cerated because ebs storage works ouside the instance.
If we delete the instance.Allthe data wouldbe gone with it.
Now,since itis an external ebs vol,we can attach it toanother instance ifwe want hence our data is also protected.

cancel form ec2 linux shell

Now goto vol--
vol1 if uwill see is in-use.
So,1st detach it.
i.e. click vol1---->actions--->detach-->yes---refresh

Now,goback to instances---terminate instance1-->

Now,wewill attach vol1 to instance2---
left pane-->volumes--->clickon vol1--->actions--->attach vol--->select instance2---->attach

goback toinstances--->click instance2--->connect  
here we just needto create amount point and start from there---
$ sudo mkdir /data
$ sudo mount /dev/xvdf /data-----cmd tomount the vol
$ lsblk
$ ls /data---------------points we created earlier exists.

i.e. what we created inthe ebs instance exists outside ofthe instance and is savedevenwhen 
we deleted the instance.

Amazon elastic file system--
After block level storage, next is file level storage.
u have folders,hierarchies hereand is good for organizing ur data.
It is alos a shared file system. Tf,multiple ec2 instances can share same filesystem.
it is elastic and scalable.completely managed by aws.
It is a true file system for linuxand doesnot work with windows instances as odf now.
Stores data in multiple availability zones, and provide us with highly durable and avialblefile system
Amazon ebs vs amazon efs--
stores data in a single availabilty zone. Vs stores data across multiple 
availbaility zones.
Both ec2 instance and ebs volume must reside wihin the same availabiltiy zone vs ec2 instance and efs volume 
can be +nt in diff availability zones.

EFS on aws--
search fro efs--
clickcreate filesystem--
name it lets say myefs--
standard means u can store ur data redundantly across multiple zones.
we willnow connect this to an ec2 instance.

launch ec2 instance-- linux2---
in no.of instances-- put 2 -- because we will see how to share file system between 2 instances.
scrolldown and see file systems--
launch system--linux_instance1,linux_instance2
now we will coneect to efs.

clickon linux_instance1---connect--ec2 instancce connect--launch----
So 1st we will create a mount point--
i.e
$mkdir data---data is a mount point
now goback to efs--click over myefs name--
attach---- copy sudo mount command---and paste on terminal--
$sudo mount -t efs -o tls fs-0a1248690986264a4:/ efs
replace efs with ur mount point i.e. data 
$sudo mount -t efs -o tls fs-0a1248690986264a4:/ data--------------------------------giving error next 3 rows-----------look
$ cd data
$ sudo mkdir analytics
$ ls

Nowgoto2nd intance--
now, goto inctance2---attach efs--same steps
after cd data
$ ls


Amazon s3 or Amazon simple storage service(s3)
s3 stores data as objects in buckets
maxi object size is 5 TB
These objects are immune i.e. aws creates versionsof objects toprotect them from accidental deletion

S3 bucket--
howtocreate s3 bucket--
search s3--create bucket--bucket name is a globaland unique name just like ur emailids. 
soif u get error with a particulr name ,that means that is created earlier.
name--bucket19112
ifu enable bucket versioning,ithas a cost attached toit.

just click over this bucket--
create folder--name b1--create folder
click over folder--upload--add file--upload some random file---
---upload--
click s3 destination button ---
The imp thing is--nme of the object is the key here---
if we upload a filewith the same name,this file willbe replaced by that file,unless we enable versioning.

Now, click on the file--- download option comes--wecan downlod itfromthis console only,
since we have bocked public access.
u can aslo access these objects through url--by copy url option.--here because we have blocked public url--its access isblocked

How tocreate s3 bucket using cli--


 download aws cli
clickon user name--security credentials--accesskeys--create new accesskey--u can have maxi of 2 access keys active orinactive.

open normal cmd prompt
$aws configure
$accesskey--AKIARNJ7ITA7CJCUKL4Q
$secret accesskey- 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
rootkey.csv is a file-----download this file
$ default region name:ap-south-1
$default o/p format:json------default is json

$ aws s3 ls
next we can run a cmd tolist the objects inside any bucket--
$ aws s3 ls bucket19112
Now,we are inside this bucket and to download any file from this bucket 
$ aws s3 cp    # we need a uri path-- so go back to aws console--search for s3 at top--clickon s3---find the bucket u r refering to--click on that-- 
---select the object---go inside the object-i.e. pptx--copy s3 url 

C:\Users\itvedant>aws s3 cp "s3://bucket19112/b1/Algorithm & Logic Building (1).pptx" C:\Users\itvedant\Downloads----done

s3 bucket policy---2tyes---identity based and reource based
adding custom policy to a bucket
how to access objects in a bucket from a specific ip addr---this is useful in case of sensitive data which can be accessed from 1
ip addr and not other.
lets see how to create--  
clcik on s3--
click on the bucket bucket19112---upload some files--click on s3;//bucket19112--permissions---here we will attach custome polisy---bucket policy--edit--bucket arn is a unique id--click on policy generator--select type of policy--s3 bucket policy
step 2-- add stmt--our aim is to allow access to the objects in the bucket from the  specific ip addr so effect is allow--in principal put *---actions--get object--
copy bucket ARN and paste in ARN---now click on add conditions--1st condition is ip addr--key in our case is aws sourceip--value in our case is ip addr--just search in google my ip adress--2405:201:1a:83:398d:a501:e958:86eb or click what is my ip addr--49.36.122.75---click add condition
this condition means-- ip addr should be from this ip---then click add stmt--click on generate policy--copy everything--head back to bucket policyy page and paste everything---save changes---some error u will get--but thats ok--in the pasted stuff-- get to resource--and put /* at the end-- ie.
"Resource": "arn:aws:s3:::bucket19112/*",---save changes--
head back to objects option---try to open up a new file--click on copy url---lets say open 1 incognito window---paste url-- done--

S3 storage classes in s3 which differs in pricing and performance--6 diff classes--
s3 standard, s3 standard infrequent access,s3 1 zone infrequent access, s3 intelligent tiering, s3 glacier,s3 glacier deep archive

s3 standard is the most premium class of s3-- its for frequently accessed data,99.999% durable,stores data in min 3 availability zones,higher cost in comparison to othere classes.

s3 stndrd infrequent access for infrequent data access but requiring high durability,also stores data in min 3 availability zones, has a lower storage price but higher retrieval price

s3 1 zone infrequ access--stores data in single availabilty zone,pricing is lower than 1st one.

s3 intelligent tiering-- for data that has changing access patterns,monitors object access patterns
eg u have data in s3 standard and it is not acceseed for 30 days, then it is transferred to s3-IA(infrequent access) class and if ur objectv starts getting freq acces then again it is transferred back to s3 standard.
s3 glacier---low cost storage,for storing archived customer records,old photos,retrieval time of objects is between few mins to hours.
s3 glacier deep archive--lower storage cost,retrieve objects within 12 hours
s3 lifecycle resources---set of rules that define actions on s3v objects. 
transformation actions-choose on when to change the storage class.
expiration actions--choose when to delete the s3 objects
so object in s3 stndrd--not accessed for 30 days-- shifted to s3-IA---not accessed for 30 days-->shifted to s3 glacier

u can maintain these types of lifecycles of the objects using s3 lifecycle policies.

how to define s3 lifecycle policy?
go to s3--click on bucket 19112---click on mgmt tab---create lifecycle rule---whenever we want to create a new obj, we want it to go to a certain class,lets say after 30 days--storage class after 60 days and automatically to get deleted after 90 days.--this is helpful in optimizing cost--so 1st name ur object--lets say-- modify-objects-every-90-days
prefix u can out like .csv,etc such that it is applicable on that particular extension file--fo now we are leaving it blank--
lifecycle rule actions--select 1st and 3rd option
in transition current versions--days after object creation--30

click add transition--this time select 1 zone-IA--60

expire current versions of objects--90

create rule--might get error--so go to choose a rule scope by scrolling up--click on this rule applies to all--click i acknowledge----click on create rule---
now this is automatically assigned to a bucket.

IAM Roles---in the IAM user account, we create policies and attach them to a particular user so that user can get specified permissions. In IAM roles where policies are not attached to the user rather the policies are attached to the aws services so that they can access the other aws services, such as ec2,s3,database,lambda.

how to define iam roles--
lets say we want to give ec2 access to s3.
create a same linux m/c instance -- in our case we have instance2--connect---
$ aws s3 ls--------error
so head back and search for iam --on the left pane click on roles---create role---we are going with default aws service--u have tonnes of services here--since we want to get ec2 access to s3-- so click on ec2--next permissions---we want to give full access to s3-- so--type s3full--select that---click next---role name--access-s3-from-ec2---create role---u r in iam console now---role that we just created is there---now go back to ec2 again--select instance2---click on actions---scroll down--security--click modify IAM role--select access-s3-from-ec2--update iam role---
now we just need to check if its working or not--
so head back to cmd line--
$aws s3 ls
if error comes---run following cmds--
sudo apt-get update
sudo apt-get install awscli

aws --version
$aws s3 ls

2023-02-27 09:54:09 bucket19112-------we got this o/p
we got the access without facing error unable to locate credentials.


Databases----------------------------
Amazon Relational Database Service(RDS)----

We generally have 2 kinds of databases--
relational db eg mysql,postgresql,
non relational db or nosql db--eg mongoDB and amazonDynamo DB

U can get all these types of databases in awsand 1 way to get this is through ec2 instance.
U caninstall any db u want and so u can have control over env variables,memory and storage capacity.
Just likeuronpremise but u get more managed service-- which is known as amazon relational db service or RDS.

So,using RDS u can run ur databases in the cloud.It support allthe db engines like--
oracle,mysql,postgreSQL,SQLServer,MariaDB,AmazonAurora..With this u get automated backups,
redundancy,failure,faster recovery,replication.,etc.

how to launch mysql RDS--
In aws console bar,search for rds--just scroll down and click create database.
standard create--mysql---version--8.0.23----templates---production--settings--
db-1 
Master username-admin
pwd--db1db1db
db instance class-forprocessing powewr and memory requirements--burstable classes has smaller--
so,select burstable classes---
under storage---generalpurpose(SSD)gp2---
allocated storage--20
storage autosclaing--25
availabilty and durability--
click donot create--in our case single db instance

connectivity--keeping default--u cn select ur own vpc too.
keep in mind--after u create db,u can not change vpc.

public access-yes

vpc security group-- for now,keep it choose existing--
existing vpc security group--default
availability zone--fornow no preference

database port--3306--keep it as it is for now.
database authentication-- we will keep as pwd.

and down uhave price calc--so for our course go for free tier.
scrolldown and see for errors--do 20 in place of200

come tothe option of vpc security group-------
open a ec2 dashboard tab in new tab---
now we will be creting security gruoup--
so in left pane of ec2 dashb search for security groups--
click create security group---
call this security_group_for_db_access
description--allows acces to the db
vpc we will keep default one selected--
for inbound--
type---search forsql--andselect mysql 
source--anywhere ipv4---

next for outbound rules---
type--mysql--destination--anywhere ipv4----
create securty group----

Now going back to rds console--
reresh rds page-- select options again--

remeber tokeep access public--
existing vpc security groups --choose security_group_for_db_access
cross default---
once done-- clickcreate database

--once its available---click ovr db-1

Connecting to mysql---nowin the trminalwewill connect tothe database we have created.

for connecting to db ,im launching a linux nased ec2 instance--
linux_instance_db key pair name

from ec2 instance i did connect--
just copy endpoint and port by clicking db-1 

so now gotoworkbench -- open workbench-- create a new connection-----

https://www.google.com/search?q=Connect+AWS+RDS+MySQL+instance+with+ec2+instance&oq=Connect+AWS+RDS+MySQL+instance+with+ec2+instance&aqs=chrome..69i57.6597j0j7&sourceid=chrome&ie=UTF-8#kpvalbx=_htP9Y774GMOKz7sPlOW-mA0_47

conne method:standard tcp/ip over ssh
ssh hostname:ec2-44-198-159-123.compute-1.amazonaws.com----------public ipv4 dns of ec2 instance
ssh username:ec2-user
ssh key file:---get path to linux_instance_db.pem
mysql hostname:db-1.cdhpw21pvxdm.us-east-1.rds.amazonaws.com--------------endpoint pf rds
port:3306
user name :admin
password:db1db1db
test connection--
when the connection is succcessful--then give conn name and save the conn for future use.
 
then connect and run-->show databases
information_schema
mysql
performance_schema
sys

Amazon Aurora---------------------------
it is aws most managed database option. Its 2 forms are-
mysql and postgresql.It iscost effective ,1/10 to other databases,
data is replicated,continuous backups to s3,point in time recovery.

How tolaunch amazon aurora using rds---
cometo rds-- create database--choose standard option---
engine type---amazon aurora---
edition--amazonaurora with mysql compatibiltiy--
version--aurora(mysql5.7)2.07.2

u dont have any free tier with this--choose production--
db cluster identifier-- db-2
master user name: admin
pwd:db2db2db

db instance class-burastable 2 gb--

with multi az deployment---2 nodesare created
write nodeand readnode,--keepit selected

public  access-yes

then go to ec2 dashboard--security groups--
click on security group for db access--
creating 1 moreinbund group--add rule---
under custom tcp--click on mysql/aurora--source--anywhere ipv4--save rules--


editoutbound rules---same
save rules---done

head back to rds--

existing vpc security groups--security-group-for_db_access--leave default as itis---
create database  

Introduction to DynamoDB--
Types of databases areppRelational DB and Non-relational DB--

Types of nosql db are-- doc based db are --
document based db like mongoDB,key-value pair based like amazon dynamoDB,col oriented like cassandra and graph based like neo4j.

DynamoDB is the amazon way of managing NoSQL db.It is a key-value db.Key is a unique identifier and value is a value associated with it.It is a serverless db.It stores this data redundantly across multiple availability zones. It can support more than 20 million requests per second.It encrypts all data by default and cretaes data backups of 100s or more tbs of data.

How to cretae dynamodb table---
search for dynamo db---and u get dynamodb console---create table--schemaless db and require only table name and a primary key--string---keep sort key unselected--- keep default settings clicked---click on create----
click on items tab---click on create item---product_id---lets say AV001--click on + sign-append----string-brand---lets say adidas--
+ sign---select---number--price---2500---save


lly, + append--
product_id String--AV002,brand String:LG,price Number : 30000,category String AC
After insertion is done-- there is a dropdown which says scan---scan--scans the entire databse and returns the filtered options .Select add filter for that-- so lets say i want to get only those items where price > 25000-- so for this:
filter  price,number >= 25000-------start search


NOw, here the task of insertion like AV001,AV002 is generally automated. and no need of performing it 1 by 1.

DynamoDB using python----
python file dynamodb.ipynb---
aws access key id and secret key acces  we need


use cases of dynamo db--
banking and finance--store user trxs,dynamo db with ml algos for fraud detection,
retail-- fro checking shopping carts,inventory tracking,customer profileseg amazon,zola,belkin,skechers,etc
media and entertainment for maintaing extreme scale of concurency and reliability. It maintains load and latency.
It stores metadata of media files,user data profiles, etc--netflix,nfl,discovery , etc
gaming technology--for clickstream data of user events,player data profiles,real time leaderboards eg zynga,pokemon company,etc

Data Warehouse--database is used to collect raw data upon which data analysis can be done.NOw everytime when we want to analyze the data using the databases,we need to preprocess it and then use it for whatever usecase we want.This data is very complex and as the data grows,it becomes more challenging.
SO what we do lets we have a trx data in our db of diff locations,payment mode,brand.We take the data from the db and process it in batches and create aggregated features that we think would be useful when we analyze our data.SO to store the trxtional data we use db and analytics team will use data warehouse.Since the aggregated results are there, it will take only the reading time and u donot have to scan the complete db for simple queries.
So, Data Warehouse is a place where we store processed data.It is also known as decision support system which has all the info in the form of which u can rely and can also do reporting and analytics.These data warehouses are further divided into data marts which are focussed on specific business parts,eg data mart of sales,data mart of employee,data mart of products 
 
It has a 3 tier architecture--Bottom tier ,middle tier, top tier.
So first of all we extract the data from multiple sources eg social media sources or any other data source.Next this data is transformed in a place known as staging area- for eg removing duplicate values,creating some extra col,etc and u summarize the data i.e. whatever u think is required for analysis. Then the processed data is loaded into a data warehouse where diff clients can use it for data mining,reporting and analytics.
i.e. Extraction is at bottom tier, staging+data warehouse at middle tier,rest at top layer.

Amazon redshift-- Its a petabyte scale data warehouse completely managed by aws.It is designed for high performance data analysis on high datasets.U can connect redshift with workbench , tableau etc. How it works--
It consists of a cluster of compute nodes.If ur cluster has 2 or more than 2 nodes than 1 cluster is assigned leader node and rest compute nodes.U can always add or delete nodes as per the requirement.Leader node coordinates with other compute nodes in the cluster and also with the client apps (like reporting tools, or any other data analytics tools.)
The interaction bet leader node and client apps is done using 2 drivers--JDBC or ODBC. JDBC is java db connectivity for java based client app.
odbc for open db connectivity
Now, whenever a leader node receive a requset from a client it creates an execution plan or series of steps to be executed.and result of the query is returned to the client in the most efficient manner.Based on execution plan it assigns the steps to the other compute nodes in the cluster.Each compute node has its dedicated cpu memory attached storage and 1 or more databases..These compute nodes are of 3 diff types-- 1 dense storage, 2 dense compute, Then RA3 nodes.U choose them during creation. 
Dense storage use HDD,4-36 virtual cpus,2 TB -16TB of storage.
Dense compute- More optimized for processing data-use SDD,2-32vCPU,160GB-2.6TB.
RA3 nodes-4-48 vCPU,32GB-128TB.It is managed by aws.
Each of the compute nodes is partitioned into slices known as node slices.Each slice is a portion of memory and disk where it performs query operations.The leader nodes distribute the data to these slices abd then they work in parallel to complete the operation.All these nodes store the data in a col fashion which drastically reduce the no. of i/p o/p operation when u r analyzing the data.

Understanding columnar storage----
1 row oriented db--to store a trxional data we genrally use a row oriented db.Generally all the dbs use block level storage as they require high end ops.We cannot use obj or file based storage for dbs.Each row is stored in a single block or more than 1 block that totally depends on the need.System reads single block in a single time.Now to get a result of no. of lg prod solds--It is goof for storage purpose but u need to compromise on performance.
In columnar storage--apart from the unique key it takes unique values in a column it stores data like-
unique value:id ege- modile:pk233,refrigerator:pk349. So it take more no. of blocks but it keeps col values together.
For another col-- next block....
so for lg products data is stored .

Creating redshift cluster--
search for redshift--click on amazon redshift--
click create cluster---cluster identifier is the unique key that identifies the cluster.--
cluster identifier-- name lets say av-cluster-1
u only get 1 cluster for free in a free trial cluster.For now select production.
size of the cluster--if u r an advanced user select i'll choose,--1st seelct i'll choose.
there are 3 types of compute nodes availbale from which we can choose,1st ra3, 2nd dense compute then dense storage.for now, select dc2 large--click on that--nodes 5--
just scroll up and select help me choose--
here u need to kno ur data is in compressed format or raw format.--leave untick my estimate is for compressed data.
size--500 gb
if u r querying data every month or some week -- then select time based--tick my data is time based--
12 months
query in ur workload--2 months
and u get the estimated price.

For now, just scroll up and select free trial.
admin username-- leave default--awsuser
admin user pwd--put any pwd.
---create cluster.

for java based eg mysql workbench use jdbc and for non java based use odbc.

Now, on the lhs , on the navigation bar-- go to query editor---click on connect to db---create a new conn---temporary credentials---cluster--av-cluster-1, db name--dev,db user--awsuser---connect
Query editor on redshift--
for the starters-- click on category--click on 3 dots--preview data--lets say we want to calc distinct category groups in my category table--
just come to query  editor--
select count(distinct catgroup) from category;---run
How to load data from s3 to redshift?
open a new instance of s3-- create a new bucket--lets say av-bucket-23--here we need to upload a file--just create 1 small file--lets say 
Sample_AV_Choice.txt
AV001,Pranav,Shoes
AV002,Lakshay,Clothes
AV003, Shveta,Books
--upload--add files-- --- upload
Now, to use a diff service-- we need to use  IAM roles.For eg here we need redshift to access s3--so,we need to define permissions.
so, open a new tab--come to IAM dashboard--left pane click on roles--create role-- aws service--
for usecase scroll down--click on red shift--red shift customizable--next permissions--search--s3read--select that--next tags---tags are optional --so next--role name-- s3-redshift-access----create role--
in a searfch bar-- just type the rolename we have given--click on that--keep this window open , we will need role arn--
now head back to ur redshift console--select av-cluster-1--actions--manage iam roles---choose s3-redshift-access--click associate IAM role--save changes

So, now we need to create a table where we can get the data. SO go to query editor--click on +---
create table sellerchoice
(id varchar(5) not null,
name varchar(20) not null,
category varchar
(20) not null);----run
click on + for new query tab--
copy sellerchoice from ''---------go to the tab where u have uploaded ur sample_AV_choice.txt file--copy s3 uri--- go back to redshift and under '' paste that.
so query be like--

copy sellerchoice from '' credentials 'aws_iam_role='------now go to IAM console--copy the role ARN, go to redshift and paste it after = in 'aws_iam_role='

copy sellerchoice from '' credentials 'aws_iam_role=' delimiter ',';--run

SO, in the left pane -- got to sellerchoice--click 3 dots--preview data

AutoScaling--to deal with inconsistent traffic-- we need to buy a stack of servers--which needs monitoring and maintenance--for that we need autoscaling
Now, how ec2 scaling works---
define the launch configuration--i.e. which ami to be used? which instance type?how much storage? which security group and private key to be used?
After this we define autoscaling group--
here we need to define min and max size of the ec2 instance that we require to handle our usecase.then launch config,autoscaling type,
3 diff types of autoscaling are-- manual scaling, dynamic scaling,predictive scaling.

for manual scaliong-- go to the dashboard and change the capacity of ec2 instance manually.
dynamic scaling--lets u define some rules for eg when cpu utilization>85% then launch an instance.
Predictive scaling- eg i only want extra resources on weekends.Here it uses ML models or schedule scaling

Project AutoScaling(Part 1)
1. define the launch configuration--
so, we are launching a simple linux instance and when cpu utilization>50% then launch a new instance.
EC2 mgmt console in aws--left pane scroll down--auto scaling-- click on launch configurations--create launch configuration
name-website-configuration
ami--we have created image of linux instance we will use it here --or otherwise quickly create it
my amis-linux instance
choose instance type--t2.micro
additional configuration--here if ur appl needs to use additional db or s3, then use IAM roles here, we can also monitor our ec2 instance usage here using cloudwatch--for now leave it
ebs volumes--
security groups-- if access required for specific ports
key pair--linuxkeys
click on I acknowledge--- create launch configuration
step 1 is done.
step 2--create auto scaling group--
go to aws 
on the left pane--just below launch configurations-- click on auto scaling groups--create an auto scaling group---
name-- my-website-auto-scaling-group
launch template---click on switch to launch configuration--select website-configuration--next
now we need to configure the n/w-- if u have a private cloud or privacy is imp in organization--then u can select ur vpc here--let it be default---for subnet-- select them 1 by 1--so what will hapeen is instances will be launched sequentially in lets say ap-south-1a,then 1-b, then 1-c,then again in 1-a and so on.
--next
for now--no load balacncer--next
step 3-- configure group size and scaling policy--it is basically asking how many instances u want aws to launch--lets say maxi capacity-10
scaling policies-- target tracking scaling policy--
scaling policy name--target tracking policy,metric type-avg cpu utilization,target vallue-50--next
notification to send to 1 or bunch of people once specific event occurs--
click on add notification--so 1st u need to define a topic i.e. under send a notification to-- put my-website-topic
with these recepients-- u can put your email-id add comma and u can add more email ids,let all 4 event ticked--next--next--create auto scaling group
Project auto scaling(part 2)--
1st go to ur email-id and confirm sunbscription
left pane--auto scaling groups--click on instance mgmt---click on instance id--select the instance--connect--ec2 instance connect--connect--
so now we will increase cpu utilization---
we will press the cmd 5-6 times just to see how it works--
yes>/dev/null &--------press this 5-6 times--
remember to close it otherwise aws bill might shoot up.
head back--goto autoscaling groups again--click on ur group--monitoring--ec2-- decrease time at the top--click 1h--just refresh it few times--so idea is if it crosses 50% then new cpu instance is launched--now go to activity--see activity history--
go to inbox--u will get few notifications--
now go to instance mgmt--2 instances are there
once u r done-- delete auto scaling group and kill the cmds at the terminal--
To kill the processes you ran to load up the CPU, please use this command - killall yes

For deleting  auto scaling groups please refer to the link below

https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-process-shutdown.html

load balancing---As the traffic grows in our website,there could be a performance issue.
Suppose we have 3 diff instance launch then how do we decide which of the active instances will handle the particular request.. so to decide this we need load balancer.It assigns req on round robin fashion to instances.And if the health of a particular instance is not good,then it send request to other healthy instances.
How to create a load balancer--Application load balancer, n/w load balancer,gateway load balancer and classic load balancer.--
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html-to study about diff kinds of load balancers.


Step 1: Configure the instance--
1st stepis to launch an instance and on that instance we will host an apache website.

so,launch instance--amazonlinux--in the security group-- we needto enable port 80 since,we are hosting a website.
so add rule--
http-->tcp--->80--->anywhere-------->review and launch--
u can choose an existing key pair--->linuxkeys------>launch nstances.
rename it to server-1----connect----
instance launched----in the zip file--commands.txt is there---just open it--4 or 5 cmds are there.
1st u need to install apache http server---
so $sudo yum install httpd
next we need tostart the service--
$sudo service httpd start
turning on the service with default configuration
$sudo chkconfig httpd on
chk status of service--
sudo service httpd status

come back to aws and copy public ip addr--
open a new tabor window --just paste it there--and u will see our website is running.

Now gotoindex.html file in folder---right click--open with other app--open it with text editor--copy everything---
go to ur editor--$sudo vi /var/www/html/index.html
u r in vim editor---press insert---just remove the sudo line---
paste everything fromhtml file.
press esc:wq

now refresh the tab of html file and u will see AWWS Server 1

Next we will launch another instance.Now,insteadof creating an instance, wewill create a copy of this instance 
select server-1i.e.click --actions--->image and templates-->create image--->name ---my-website---create image
in the left hand pane -->scroll down-->clickon AMIs--
now go back to instances-->launch instance--->now instead of selecting linux istance, clickon my AMIs---->my-website--->select that --
in security group---->add rule--->http,tcp,80,anywhere,reviewand launch--select existing key pair  
(linuxkeys)---launch instance
name it--server-2----connect toit---copy thepublic ipaddr--
paste itin a new tab orwindow----o/p---AWS Server 1
just goto ec2connect---connect----noneed of witing other cmds--just write--
$sudo vi /var/www/html/index.html
press insert
the only change u need tomake is--instead of aws server 1 write AWS Server 2
esc+:wq
just goto 2nd server tab --refresh it--AWS Server2 aso/p

Step2: Configure the load balancer--Configure Listeners and TArget Groups

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html

Go to  ec2dashboard---left pane--loadbalancers---click---create load balancer---
4 typesof load balancers arethere--- Application,Network,Gateway,Classic

Click create on app load balancer--

nmae:my-website-load-balancer
scheme-- if u r working on a private network--then keep it internal,ours is public,so keepinternet facing--
ip addr :ipv4 let it

listener is a process which checks for conn requ--
let it

availability zones--select minof2 from here--lets say 1-aand 1-b
--next---getting warning because we used http and not https.We will need a ssl certificate which we donot have right now---
click next--open a new aws tab --instances--select any instance--lets say server-1----scrolldown and see security groups--see tis security group name in configure securiy groups window--select it
---next---configure routig--
target group--new target group
name:sample-target
health checks-- protocol--http
path /var/www/html/index.html--------path wherewe are routing the traffic
register targets---select both server-1 and server-2 and add to registered
--next--create

Now,coming to step3--test the load balancer---
select the load balancer--scrolldown--copy the dns name into new tab--paste-- we got AWS Server 2
refresh the page andu get AWS Server 1--i.e.the traffic is getting  balanced between 2instances.

Now,step4: Attach a Web Application Firewall
goto aws--search for waf--WAF and Shield--lets say we want to accept traffic froma certain regionand not from other--
click create web ACL--
name: my-rule-for-lb
now region should be same as load balancer--Asia PAcific Mumbai
Associate awsresources--clickon add--
3types-- amazon API Gateway,Application Load Balancer,AWS AppSYnc
Select Appload balancer---click my-website-load-balancer--next
we want to add a rule toonly allow traffic from India--click on add rules---add my ow rule and rule groups--
3 rule types-- ipset,rule builder,rule group--select rule builder---
scroll down--name tis rule as--only-allow-in-India
rate based rule is for eg-- after 100 imes deny,etc--select regular rulein our case
if a request-matches the stmt
inspect--originates from a country in
country codes--India-IN
keep sourceip addr selected
action--allow
-------------add rule---
default web action--block--next---next---next----create web ACL
sinc,itisa paid service-- for every rule--uhave topay 1 dollar perrequest charges are there as well

lets say iopen a chromeextention in a new chrome window--1st download and install--
ipunblock.com/browservpn/
we will change our location and send a req then it shouldbe blocked-
in the rigt pane--select UK(London)
just typemy locationin google--
now,go back tothe load-balancer website pagetab--copy this addrand in our new window where we changed our loc--paste it
while in normal browser , this site is opening

Management and Governance---
compute using ec2,private n/wusing vpc,iamuser acc to define permissions,
Now we willlearn how tomonitor the resources in an AWS enviand we will crete dashboards using cloudwatch
cloud trailis the auditing service that helpsus create logs--trusted advisor 

ThenAWS lambda--it is the serverlesscomputing provided by aws

Then we will see Pricing and see how to create AWS budgets.

Then migration-u already have an apphosted on premises and u want to host it on cloud
then what are the challenges u will face.How does AWS Cloud adoption framework helps us,Migration Strategies

Amazon Cloudwatch--
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-services-cloudwatch-metrics.html

Cloudwatch allows u to monitor and manage ur resources on AWSin realtime.
U can setalarms and visualize logs,u can take automated actions and troubleshoot issues and discoverinsights tokeep ur apprunning.

Using cloudwatch we 1st collect datapoins andlogs from diff aws resources like ec2,database,etc
then we monitor & visualize those datapoints using cloudwatch dashboardsand we can even assign somealarnms
after this if anything goeswrong then u can act usingautomated responsesand then finally u can analyze.
cloudwatch concepts--
for eachof the AWS service that we need to create--we need a namespace
sothe naming convention is like--AWS?DynamoDB,AWS/EC2,etcin the link u will get
lets say weuse AWS/EC2--then use metrics--CPUUtilization,NetworkPacketsIn
Every metric has some characteristics which is called as Dimensions
foreg if u want to track cpu utilization,but for which instanceid.U can upto 10dims for a metric.
Next is statistics--  eg avg,min and max cpu utilization.
After that we have alarms which u can customize.

How to create cloudwatch dashboards--
search forec2--select 1 of the instances---scrolldown click monitoring--this data is specific to this particular instance
open another aws instance--search for cloudwatch--on left click dashboards---create dashboard---name: aws-course-dashboard--create dashboard
cross that which popped.
head back to ur ec2 instance--we want totrack cpu uilization for my-instance--click on 2 dotsi.e. widget actions--
--add to dashboard---new tab opens-- select aws-course-dashboard---click on line---add todashboard--save dashboard
another way is--click on add widget---select the widget u want lets say line--next--metrics---configure
u can name the graph--Network Packets In--scroll down---select ec2 instance---perinstance metrics---search for network --
it shows ourinstance NetworkPacketsIn--select tht--create widget--save dashboard

lets head back to ec2 instance---select my-instance--connect--connect---
$ ping google.com---we can track this in our dashboard--this dashboard is updated every 5 mins

now,create new widget and networkpacketsout----afeter searching and selecting my-instance networkpacketsout--goto graphed metrics tab and u can choose 
statistic,period-------creae widget--save dashvoard

Setting up cloudwatch alarm-On the dashboard--on left --clickon all alarms--create alarm--when cpu utilization for an ec2 instance goes below 10%.
click select metric---ec2--preinstance metric--search bar--cpu utilization--click on my-instance---select metric--
period-1min---conditions--threshold type--static--whenever cpu utilization is--lower/Equal---than 10--next---10isfor 10%
sns--simple notiifcation service---
snstopic--create new topic--cpu-alarm--u can give ur email--create topic--
ec2 action---add ec2 action---in alarm--means alarmtriggered,ok--means everythings fine,insufficient data when instance cant be reached do u want to trigger alarm--
choose--in alarm--stop the instance--next---
name: EC2-CPU-Alarm--next---create alarm---

now head back to ec2 instnce---click on instances--->u will see my-instance is running
in the email check for aws notification--confirm subscription 
---goto cloudwatch>alarms tab---click refresh--in -alarm--emeilwill reach u.

Amazon CloudTrail---Another monittoring service. Any incident that might effect ur environment like IAMuser deletion or anything.
As an admin u want tofigureout what went wrong. SO in that case,AWS CloudTrail can helpu.
Itis a comprehensive auditing rule.Every reqon ur AWS envi gets logged into cloudtrail.

How touse Amazon cloudtrail----
aws console--search cloudtrail--by default all the log files arestored for 90 days.
morethan that is charged.
How to create custom trail---click on trails in left pane--create trail--
name:demo-trail
create new s3 bucket
name:demo-trail-bucket
--next---choose log events--mgmt event(captures IAM role,orIAMpolicy),data events,insights events---
tickallof them---
data event--dynamoDB---If u want tocheck for all the dynamoDB tables then let read write clicked in data event-
but forspecific dynamodb table--uncheck both--look for the relevant tableand accordingly select read orwrite.

for now we r doing forallthe tables--next
--create trail
tis willtake sometimetoget the trail.

in aws dashboard in other tab--search for s3--lets say we want tocheck trailfors3 bucket--goinside--inside--tilllog--

Trusted Advisor--
Itis a service that inspects ur AWS envi and provides real time recommendations in accordance with AWS best practices.
cost optimization,performance,security,faulttolerance,servicelimits
For each of these it givess 3 diff metrics--
No probs detected,investigation recommended,actions recommended.

How touse trusted advisor---
aws console--trusted advisor--clikc on 1st option--

AWS Lambda---
Serverless computing--Whilelaunching an ec2 instance we need totk care of alot of stuff like os,comp power and memory,storage
etc.In the serverlesscomputing,u jut need toupload the code and the backend our cloud serviceprovider will take care of h/w.
It willtake care of scalability challenges.Its usecase are online fileprocessing.If u  havea website where u convert a doc into 
pdf and u r not sure about the traffic.Sowhat u can do is deploy ur code and it will be taken care.
2nd use APIs--app programming 
 data and analytics--Supposeu r creating an analytics app and u r saving ur data in dynamodb,now whenever u read write or updatein a table 
an itemin a table , an event is geenrated.What u can do is define ur customemetric, and upload urcode in serverless computing.
etc

How AWS Lambda works?---
It is a serverless compute service that lets u run code without provisioning or managing servers.

First u need to define a func in any of the 6 lang i.e. ruby,java,go,C#,python,node.js
next define when u want this func tobe triggeredi.e. setup ur code to trigger fromhttp endpoints ,in-app activity and other aws services.
once the codeistriggered , lambda runs the code using required compute resources.
in the end,u just need topay forthe compute time used. 

AWS lambda pricing--Pay for what u use-- requests+duration.
1million free requests per month. 3.2 million free compute seconds per month
after 1 million requests ,u r charged $0.20 per 1 Million requests.

Creating APIs using lambda functions--
aws conole--search lambda--create function -- 4 ways--
author fromscratch,use a blueprint,container image,browse serverless app repository
select author form scratch--
name: sample_lambda_demo
runtime:python3.9
create funcion
now toexecute this func, weneed to have trigger--click on add trigger--api gateway--api--create an api--
api type--rest api---security--open--add

under configuration-left pane triggers-- this api is added--clickon details--open apiendpoint in a new tab--
u will get hello from lambda.Go back to code and in function u can make changes like hello from Parinita---changes deployed

Now how we can print the parameters that are passed with the APIin thelambda func.In that case we will use api parameters.
goto configuration--api gateway--
clickon sample-lambda-demo--u can see someofmethods i.e. api methods--clickon method request----url query string parameters--
add query string---name---check required
head back tolambda func---code---nowwhen the api is called,itis called with a name parameter.
def lambda_handler(event,context):
    print(event)-------------------------change made
    return {
        "statuscode" : 200,
        "body":json.dump{"Hello from AV"}}---deploy
 
configuration---click that link----this timein a new tab with a new url--we will pass 
additional parameter that we crreated,i.e.name parameter--
so,in url at the end type ?name=Parinita
the o/pin this page will remain same.
Now,going back to lamda func.1st goto monitoring tab--clickon view logs--
see the latest log stream--click onthat--- see for "queryStringParameters":{'name':'Lakshay'} 

going back tolambda func code--

def lambda_handler(event,context):
    name=event['queryStringParameters']['name']
    sting_output="Hello from "=name
    return {
        "statuscode" : 200,
        "body":json.dumps(string_output)
}
--deploy

pass the name parameter again in url.
"Hello from Parinita"

Best practices with AWS lambda--
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

Avoid using recursive functions in aws lambda. minimize complexities of ur dependencies,
use environmentalvariables.
ther are also some limits and quotas fro aws envi--
ucan have upo concurrent executions--1000by default
memory allocation--120MB to 10240MB(in 1MB increments)
Timeout--900 seconds

AWS Pricing Cncepts--
https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/
3 diff pricing are-- pay as u go,save when u reserve, pay less by using more
pay as u go--only pay for the servces u consume.No additionalcosts or termination fees.
save when u  reserve--lower prices for specific usage commitment.diff saving plans are there.
pay less by using more--

Billing Dashboards--
search billing or gotouser name-- clickon my biling dashboard--
u get billing and cost mgmt dashboard--

bills--

AWS budgets--A budget is an estimation ofur expenses over a speciifed future period oftime.
This hlps in creating custome budgets to track your cost.Granular budget time periods.Set custom alerts when ur usage exceeds.
So u can monitor ur cost andusage.U canschedule reportwithot needing tologinto ur aws account.
U can respond in time and u can also set some customaction plans.

How to create an aws budget--
aws console-- search budgets--aws budgets--crate budget--
4 typesof budgets are cost budget-recommended,usage budget,savings plans budget,reservation budget

cost budget--next--period forwhich u want to set budget--monthly
if budget is recurring every month then choose recurring else expiring.
select expiring for now--starts--sep2021, ends--oct2021
fixed--
amount 30.00 
details--name:budget for fixed period
alerts--add an alert threshold--
eg if i cross 50% of my budget amount trigger--So,
50 %of budget amount trigger-actual
write email recipents
+ add alert threshold-
80 %of budget amount trigger-actual
email recipients-next

attach actions--
alert #1--add action--u can create IAMrole(name budget-for_ec2)
action type to be applied--automate instances to stop for EC2 or RDS 
stop EC2
region--asia pacific mumbai-ap-south-1
select the instance--currently there is none
lly,do for alert2--next--if u get error just remove those--then click next
--create budget

AWS Cost Explorer---
Its a consolebased service wich visually allows us to 
analyze hw we r spending money.
aws console--cost explorer---aws cost mgmt--in left pane-costexplorer--


You can use free tier services of AWS and learn different services. You can check the link given below for free tier eligibility and use the resources accordingly.

https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all

Challnges before migration--
Reason formigration can be many--increasing workloads,security,performance.
and there are tonnes of ques. Toansthat we have cloud adoption framework.

AWS cloud adoption framework--It helpsus idenify key cloud adoption activities and helps us accelerate 
ur journey to the cloud.
It has 4 stages--envisiom,align,launch,realize value
envision workshop--prioritize goal define success metrics, helps u create a fondation for ur cloud strategy.

Align--it will result in an executable action plan,resolve concerns related to diff stakeholders,establish best approach.

launch--prioritize projects,define and execue worksreams,deliver outcomes

realize value--ensure business outcomesmeet expectations,measure incremental business vlue,identify additional cloud projects.

cloud migration strategies--
6 typesare-- re-host,re-platform,re-factor,re-purchase,retire,retain

rehost--move appto cloud without any modification.
here allthe binary files,data,os files goas it is tocloud.
just changein the infrastructure.
speeed of migration,several tolls like aws app migration service,aws server migration service
its dadv--u maynot be able to take adv of other AWS services.

re=platform--core architectureis kept same.
i.e.u take binary files,data as it is but on cloud u use diff version of OS.
adv-take adv of cloud managed services,switching fromcommercial s/w to open-source
dadv-time consuming,migh need to run both on-premise and cloud env in parallel.

re-factor--almost a new app is created.U only take data with u.
adv-new app uses cloud native features.Increased efficiency and agility
dadv-could be complicated and take more time to migrate,could be expensive

AWS Sonwfamily---
whilemigrating to cloud,most of the timeis consumed on transferr of data.
eg totransfer 100tb of data with 50mb/sec can require 55+hrs 
Awssnowfamily is a collection of physical devices hatelpmigrate large amounts ofdata into 
and out of the cloud without depending on n/ws.
types--
aws snowcone-it is the smallest member.can store upto 8 tb of data,edge computing--
awssnowball- 2 ypes--storage optimized, 80TB HDD,1TB SSD,80 GB RAM and computeoptimizd 42TB HDD,7TB SSD,
208 GB RAM
awssnowmobile--100PB,highly secure
aws snowconole--place order for any of these devices--aws send required device to urlocation--
then u can connect ur devic toaws snow device--do data copy or compute as per ur needs--
ship back the device to data center--data is then copied to s3 bucket
and then the data willbe erasedfromsnow devices.Throughout this journy data is secured with 256 bit encryption
andonly u have the key to unlockit.

aws certiifcate link

https://courses.analyticsvidhya.com/certificates/li0fbqarnj




  






   
